\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {2.1}{\ignorespaces Cumulative distribution of (left) hop counts between pairs of host-server IPs that share the first, first two, or first three bytes, and (right) standard deviation of hop count distribution among groups of IPs sharing the first, first two, or first three bytes. The more similar two IPs are, the closer they are and the more similar their distances to the same third IP are.\relax }}{9}{figure.caption.3}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces Generating a normalized IP address for 192.168.133.130/20.\relax }}{11}{figure.caption.4}% 
\contentsline {figure}{\numberline {2.3}{\ignorespaces The neural network used for training our embedding model. The first eight layers receive the normalized IP addresses as input and compute the IP representations. The ninth layer estimates the hop count between two IP addresses and the tenth layer measures the model error. Elements in red are input. For simplicity we depict the input as one-dimensional vectors (one normalized IP); in reality, all inputs are matrices.\relax }}{12}{figure.caption.5}% 
\contentsline {figure}{\numberline {2.4}{\ignorespaces (left) Cumulative distribution of cluster similarity, computed using IP vector representations, for prefix-based and end-host random clusters; (right) Cumulative distributions of absolute distance estimation errors for DIP{} and {\em mean}. DIP{} representations preserve real-world prefix-based clustering and predict distances accurately.\relax }}{16}{figure.caption.6}% 
\contentsline {figure}{\numberline {2.5}{\ignorespaces Map-based spoofing detection. Network maps detect only spoofed packets traversing protected ASes (colored in green). Host maps detect only packets spoofed with IPs present in the map (also colored in green). Learned host maps have the ability to detect all packets because they learn missing map entries. The paths in the diagrams indicate the apparent source of the packet (the spoofed source). In reality, all packets originate from the attacker.\relax }}{25}{figure.caption.8}% 
\contentsline {figure}{\numberline {2.6}{\ignorespaces The learning-based spoofing detector uses an embedding of the Internet to estimate hop count information to any IP address and detect when the IP is used as the spoofed source of an attack packet.\relax }}{28}{figure.caption.9}% 
\contentsline {figure}{\numberline {2.7}{\ignorespaces Coverage for (left) exact and learned maps for 1,000 source IP addresses, and (right) learned maps for the same sources used in training (labeled ``1/1''), ten times as many sources (``1/10''), and a hundred times as many sources (``1/100''). The bars represent the error of a learning-based spoofing detector, for each target, in hop counts. Learning-based spoofing detectors adapt well to new sources with little loss of coverage.\relax }}{31}{figure.caption.11}% 
\contentsline {figure}{\numberline {2.8}{\ignorespaces Detector accuracy under various scenarios. (left) We run detection on the same targets used in training and vary the detection threshold; increasing the threshold reduces sensitivity and improves specificity; (right) We perform detection using both training targets and new targets not used in the training process; we set the detection threshold to two hops (the average estimation error of the model); as we vary the ratio between training and detection targets, the sensitivity for the new targets is comparable to that of the training targets, while the specificity is lower; ``all'' indicates that we perform training and detection on all targets, therefore there are no new targets.\relax }}{34}{figure.caption.12}% 
\addvspace {10pt}
\contentsline {figure}{\numberline {3.1}{\ignorespaces Baseline pipeline for domain or intent classification.\relax }}{41}{figure.caption.16}% 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Direct models evaluation pipeline.\relax }}{41}{figure.caption.17}% 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Integration of $n$-best hypotheses with two possible ways: 1) concatenate hypothesized text and 2) concatenate hypothesis embedding.\relax }}{42}{figure.caption.18}% 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Improvements on important domains.\relax }}{47}{figure.caption.22}% 
\contentsline {figure}{\numberline {3.5}{\ignorespaces The influence of different amount of hypotheses.\relax }}{48}{figure.caption.23}% 
\addvspace {10pt}
\contentsline {figure}{\numberline {4.1}{\ignorespaces Model accuracy\relax }}{64}{figure.caption.25}% 
\contentsline {figure}{\numberline {4.2}{\ignorespaces Training time.\relax }}{64}{figure.caption.25}% 
\contentsline {figure}{\numberline {4.3}{\ignorespaces Amplifying factor vs learning rate\relax }}{64}{figure.caption.25}% 
\contentsline {figure}{\numberline {4.4}{\ignorespaces PTB: Negative sampling 5.\relax }}{67}{figure.caption.26}% 
\contentsline {figure}{\numberline {4.5}{\ignorespaces PTB: Negative sampling 15.\relax }}{67}{figure.caption.26}% 
\contentsline {figure}{\numberline {4.6}{\ignorespaces Different amplifying factor.\relax }}{67}{figure.caption.26}% 
\addvspace {10pt}
\contentsline {figure}{\numberline {5.1}{\ignorespaces Working paradigm comparison between custom distributed Spark-Based Datalog platforms and the LLib.\relax }}{77}{figure.caption.27}% 
\addvspace {10pt}
