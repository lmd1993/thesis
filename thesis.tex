\documentclass [PhD] {uclathes}
%\documentclass [PhD,draft] {uclathes}

\renewcommand{\floatpagefraction}{0.7} 
\renewcommand{\dblfloatpagefraction}{0.7}
%\usepackage[utf8x]{inputenc}

\usepackage{natbib}

\usepackage[
pdfauthor={Mingda Li},
pdftitle={Mingda Li -- PhD Thesis},
pdfsubject={Learning Deep Neuromuscular Control of the Torso for Anthropomimetic Animation},
pdftex, pagebackref, pdfpagelabels, hypertexnames, bookmarks, bookmarksnumbered,
plainpages=false,
naturalnames=false,
pdfpagemode=UseOutlines,
]{hyperref}

\hypersetup{
colorlinks,
citecolor=[rgb]{0.0,0.3,0.0},
filecolor=[rgb]{0.6,0.0,0.0},
linkcolor=[rgb]{0.0,0.0,0.4},
urlcolor=[rgb]{0.0,0.0,0.4}}

\usepackage{setspace} % single-spaces figure cations
\usepackage{multirow}
\usepackage{booktabs} % For formal tables
\usepackage{amsmath, amsthm, bm, amssymb, gensymb}
\usepackage{tikz} \usetikzlibrary{positioning}
\usepackage{graphicx, caption, subcaption}
\usepackage{adjustbox}
\PassOptionsToPackage{draft}{graphicx}
\graphicspath{ {figures-new/}{figures/} }
\usepackage[utf8x]{inputenc} 
\usepackage{romannum}

%%%Negative Sampling
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\urlstyle{rm}
\def\UrlFont{\rm}
\frenchspacing
\usepackage{amsthm}


% All commands to check here added by Mingda
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{appendix_theorem}{Theorem}
\usepackage{amsmath,stackengine}
\usepackage{amssymb}
\renewcommand\qedsymbol{$\blacksquare$}
\usepackage{booktabs,subcaption,amsfonts,dcolumn}

\usepackage{latexsym}
\usepackage{balance}
\usepackage{bbm}
\usepackage{url}
\usepackage{mathtools}% Loads amsmath
\usepackage{tablefootnote}

%below library is added by Zijun
\usepackage{breqn} 
%%%Negative Sampling End




\usepackage{color} \definecolor{new-text-color}{rgb}{0,0,255}
\newenvironment{NEW}{\color{new-text-color}}{}


\def\Arrow{\makebox[6.26399pt]{$\blacktriangleright$}~}
\def\Square{\makebox[6.26399pt]{$\blacksquare$}~}
\newcommand{\mat}[1]{\bm{#1}}
\renewcommand{\vec}[1]{\bm{#1}}
\def\diag{\mathop{\mathrm{diag}}}


\usepackage{url}
\usepackage{enumerate}
\usepackage[detect-all]{siunitx}



%%% LLIb

\newcommand{\bldl}{\[\begin{array}{ll}}
% \newcommand{\cldl}{\vspace*{-0.3cm}\[\begin{array}{ll}}
\newcommand{\cldl}{\[\begin{array}{lrcl}}
\newcommand{\eldl}{\end{array}\]}
%\newtheorem{lemma}{Lemma}[section]
\usepackage{mathptmx}
\usepackage{algorithmic, amsmath,graphicx}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{booktabs,array}

\newcounter{myrow}
%%% Macros for the guide only %%%
\hyphenation{either}
\providecommand\AMSLaTeX{AMS\,\LaTeX}
\newcommand\eg{\emph{e.g.}\ }
\newcommand{\qr}[1]{\textit{Query #1}}
\newcommand{\ex}[1]{\textit{Example #1}}
\newcommand{\Bi}[1]{\ensuremath{\mathcal{B}_{#1}}\xspace} % the MBS of a partition
\newcommand{\LB}{\ensuremath{\textsf{LB}}\xspace}
\newcommand{\UB}{\ensuremath{\textsf{UB}}\xspace}
\newcommand{\lowb}[1]{\ensuremath{\textsf{LB}_{\mathcal{B}_{#1}}}\xspace}

\newcommand{\return}{\textsf{return}\xspace}
\newcommand{\topk}{top-\ensuremath{k}\xspace}
\newcommand{\simmetrics}{\ensuremath{\sim}\xspace}
\newcommand{\bigo}{\ensuremath{\mathcal{O}}\xspace}

\newcommand\etc{\emph{etc.}}
\newcommand\bcmdtab{\noindent\bgroup\tabcolsep=0pt%
	\begin{tabular}{@{}p{10pc}@{}p{20pc}@{}}}
	\newcommand\ecmdtab{\end{tabular}\egroup}
\newcommand\rch[1]{$\longrightarrow\rlap{$#1$}$\hspace{1em}}
\newcommand\lra{\ensuremath{\quad\longrightarrow\quad}}

%%% LLib ends

\DeclareOldFontCommand{\bf}{\normalfont\bfseries}{\mathbf}

%% dip start

%\usepackage[utf8]{inputenc}
\usepackage{amsmath,stackengine}
\usepackage{color}
\usepackage{colortbl}
%\usepackage{ntheorem}
\usepackage{graphicx}
\usepackage{balance}
\usepackage{tablefootnote}
\usepackage{booktabs,subcaption,amsfonts,dcolumn}
\usepackage{multirow}
\usepackage{indentfirst}
%\theoremseparator{:}
\newtheorem{hyp}{Hypothesis}
%\newcommand{\system}{WASP}
\newcommand{\ie}{{\em i.e.}}

\newcommand{\etal}{{\em et al.}}
\newcommand{\system}{DIP}
\newcommand{\todo} [1]{\textcolor{blue}{{\sf TODO}: #1}}
\newcommand{\sele} [1]{\textcolor{blue}{{\sf CHOOSE}: #1}}
\newcommand{\mingda} [1] {\textcolor{orange}{{\it MingdaCamera}: #1}}
\newcommand{\lume} [1] {\textcolor{blue}{{\it Lume}: #1}}


%%% dip ends


\usepackage{tikz}
\usetikzlibrary{positioning}

\title{Extracting Latent Semantic Information from Multi-domain Contents by Decalaritive Language and Efficient Algorithm}
% Extracting latent semantic information from abandoned contents/sequences 
% Advanced analytics of abandoned contents/sequences  via declaritive language and efficient algorithm

\author         {Mingda Li}
\department     {Computer Science}
\degreeyear     {2020}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chair          {Junghoo Cho}
\chair			{Carlo Zaniolo}
\member         {Yizhou Sun}
\member         {Yingnian Wu}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\dedication     {\textsl{To my mother, father and girl friend.}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\acknowledgments {
	
First and foremost, I would like to thank my  advisors, Prof. Junghoo Cho and  Prof. Carlo Zaniolo.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vitaitem   {2011-2015}
{B.S. Computer Science and Technology\\
Harbin Institute of Technology\\
Harbin, China}
\vitaitem  {2015–2016} 
{Research Assistant\\
ScAi Laboratory\\
University of California, Los Angeles\\
Los Angeles, California}
\vitaitem   {2016–2020}
{Teaching Assistant\\
Computer Science Department\\
University of California, Los Angeles\\
Los Angeles, California}
\vitaitem   {2016}
{Research Intern\\
	Teradata\\
	Los Angeles, US}
\vitaitem   {2017}
{Research Intern\\
	NEC Lab\\
	Princeton, US}
\vitaitem   {2018}
{SDE Intern\\
	Amazon AWS Redshift\\
	Palo Alto, US}
\vitaitem   {2019}
{Applied Scientist Intern\\
	Amazon Alexa AI\\
	Boston, US}
\vitaitem   {}
{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\publication{Mingda Li, Weitong Ruan, Xinyue Liu, Luca Soldaini, Wael Hamza, Chengwei Su. ``Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses.'' ArXiv 2020. To be submitted to \emph{SLT 2021}}

\publication{Mingda Li*, Zijun Xue*, Junghoo Cho. ``Amplified Negative Sampling: Sample-Efficient Training for a Large-Class Classifier.'' In review by the \emph{KDD 2020}}

\publication{Jin Wang, Chunbin Lin, Mingda Li, Carlo Zaniolo. ``Boosting Approximate Dictionary-based Entity Extraction with Synonyms.'' Accepted by \emph{Information Sciences Journal 2020}}


\publication{Mingda Li, Weitong Ruan, Xinyue Liu, Luca Soldaini, Wael Hamza, Chengwei Su. ``Multi-task Learning of Spoken Language Understanding by Integrating  N-Best Hypotheses with Hierarchical Attention.'' To be submitted to \emph{COLING 2020}}

\publication{Mingda Li, Jin Wang, Youfu Li, Carlo Zaniolo. ``LLib and LFrame: Logical Libraries and DataFrames for More Expressive Logical Programming.''  To be submitted to \emph{ICLP 2020}}


\publication{Jin Wang, Jiacheng Wu, Mingda Li, Jiaqi Gu, Ariyam Das, Carlo Zaniolo. ``Formal Semantics and High Performance in Declarative Machine Learning using Datalog.'' In review by \textit{VLDB Journal 2020}}

\publication{Mingda Li, Cristian Lumezanu, Bo Zong, Haifeng Chen. ``Learning-based spoofing detection.'' In review by the \emph{EuroS\&P WTMC 2020}}

\publication{Jin Wang, Chunbin Lin, Mingda Li, Carlo Zaniolo. ``An Efficient Sliding Window Approach for Approximate Entity Extraction with Synonyms.'' Accepted by \emph{EDBT 2019 }}

\publication{Carlo Zaniolo, Ariyam Das, Jiaqi Gu, Youfu Li, Mingda li, Jin Wang. ``Monotonic Properties of Completed Aggregates in Recursive Queries.''  \emph{ArXiv 2019}}

%\publication{Cristian Lumezanu, Nipun Arora, Haifeng Chen, Bo Zong, CHO Daeki, Mingda Li. ``Network endpoint spoofing detection and mitigation.'' US Patent App. 16/101,815, 2019.}

\publication{Ariyam Das, Youfu Li, Jin Wang, Mingda Li, Carlo Zaniolo. ``BigData Applications from Graph Analytics to Machine Learning by Aggregates in Recursion.'' Accepted by \emph{ICLP 2019}}

%\publication{Cristian Lumezanu, Nipun Arora, Haifeng Chen, Bo Zong, CHO Daeki, Mingda Li. ``Network endpoint spoofing detection and mitigation.'' US Patent App. 16/101,815, 2019.}


%\publication{Cristian Lumezanu, Nipun Arora, Haifeng Chen, Bo Zong, CHO Daeki, Mingda Li. ``Neural network based spoofing detection.'' US Patent App. 16/101,794, 2019.}

%\publication{Cristian Lumezanu, Nipun Arora, Haifeng Chen, Bo Zong, CHO Daeki, Mingda Li. ``Network gateway spoofing detection and mitigation.''US Patent App. 16/101,834, 2019.}



\publication{Mingda Li, Cristian Lumezanu, Bo Zong, Haifeng Chen. ``Deep Learning IP Network Representations'' \textbf{BEST PAPER AWARD} of  \emph{ACM SIGCOMM Big-DAMA 2018}}

\publication{Mingda Li, Cristian Lumezanu, Bo Zong, Haifeng Chen. ``Learning IP Network Representations.'' Accepted by \emph{ACM SIGCOMM CCR 2018}}

\publication{Zijun Xue, Ruirui Li, Mingda Li. ``Recent Progress in Converesational AI.'' \emph{KDD Conversational AI workshop 2018}}


\publication{Youfu Li, Mingda Li, Ling Ding, Matteo Interlandi . ``RIOS: Runtime Integrated Optimizer for Spark.'' Accepted by \emph{SOCC 2018}}

\publication{Muhammad Ali Gulzar, Matteo Interlandi, Xueyuan Han, Mingda Li, Tyson Condie, Miryung Kim. ``Automated Debugging in Data-Intensive Scalable Computing.'' Accepted by \emph{SOCC 2017}}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\abstract{
	With large amounts of data continuously generated by intelligence devices,  efficiently analyzing huge data collections to unearth valuable insights has become one of the most elusive challenges for both academy and industry. 
%	both academy and industry are motivated to develop powerful algorithm
	%the big data analytics  to unearth valuable insights from huge data has motivated both academy and industry to develop scalable algorithms  . 	
	The key elements to  establishing a scalable analyzing framework should involve a well-proposed algorithm to integrate all available information, an optimal training strategy  and a declarative developing interface. In this dissertation, we focus on the comprehensive enhancement through the scalable analyzing framework by  (\romannum{1})  domain-specific framework designs to enrich the captured information, (\romannum{2})  a sample-efficient training method adaptive to a wide variety of multi-class classifiers with extreme large output-class size, and (\romannum{3}) a cross-language interface for succinct expressions of recursions  in advanced analytics.  
	
	
	
	%A series of challenges are contained in each of the three elements, which we want to address. 
	Our contributions in this thesis are thus threefold: First, we aim to utilize  variants of recurrent neural network (RNN)  to incorporate some enlightening sequential information overlooked by the conventional works in two different domains including Spoken Language Understanding (SLU) and Internet Embedding (IE). In SLU, we address the problem caused by solely relying on the first best interpretation (hypothesis) of an audio command through a series of new architectures comprising bidirectional LSTM and pooling layers to  jointly utilize the other hypotheses'  texts or embedding vectors, which are neglected but with valuable  information missed by the first best hypothesis. In IE, we propose the DIP, an extension of RNN, to build up the internet coordinate system with the  IP address sequences, which are assigned hierarchically and encode structural information of the network but are also unnoticed in conventional distance-based internet embedding algorithms. Both DIP and the integration of all hypotheses bring significant performance improvements for the corresponding downstream tasks.
	Then, we investigate the training algorithm for multi-class classifiers with a large output-class size, which is common in deep neural networks and typically implemented as a softmax final layer with one output neuron per each class. To avoid expensive computing the intractable normalizing constant of softmax for each training data point, we analyze and enhance the well-known negative sampling to the amplified negative sampling algorithm, which gains much higher performance with lower training cost. Finally, for the ubiquitous recursive queries in advanced data analytics, on top of BigDatalog and Apache Spark, we design a succinct and expressive analytics tool encapsulating  the functionality and classical  algorithms of Datalog, a quintessential logic programming language.  We provide the Logical Library (LLib), a Spark MLlib-like high-level API supporting a wide range of Datalog algorithms and the Logical DataFrame (LFrame), an extension to Spark DataFrame supporting both relational and logical operations. The LLib and LFrame enable smooth collaborations between logical applications and  other Spark libraries and cross-language  logical programming in Scala, Java, or Python. 
	
	
	
	
	
	
	
%    In the first part, we aim to utilize the variants of recurrent neural network (RNN)  to incorporate some enlightening sequential information overlooked by the previous works in different domains and tasks. 
%    We find, 
%%	However, in reality, we may find the provided algorithm could overlook  or  hardly integrate some enlightening information within its framework. For e.g., 
%	in Spoken Language Understanding, only the best of automatic speech recognition (ASR)  interpretations (hypotheses) for  an input audio signal is utilized to understand the intent, while the rest hypotheses containing fragmented important messages are ignored. We investigate a series of  methods to jointly utilize top $n$ interpretations by integrating the hypothesized text or hypothesis embedding vectors with BiLSTM and achieve significant accuracy improvements for intent or domain classification.   
%	 Similarly, while embedding  the Internet structure,  only the distances among IP address are utilized to build up the network coordinate system  but the information contained by the  IP address  is unnoticed. 
%	% Since the RNN and its variants have been proven to be effective to capture causal and/or contextural information from sequantial data, we extend them to incorporate the information from sequences of spoken words or IP bits. 
%	We propose the DIP, a deep learning based framework for IP network representations, which normalizes each IP address to seperate sequences by the volume of contained routable information within IP bits and exploits a variant of RNN for a low-dimensional representation.  
%	
%	In the second part, we investigate the training algorithm for multi-class classifiers with a large output-class size, which is common in deep neural networks and typicaly implemented as a softmax layer in the final layer 
%	 %like natural language generation with  a softmax layer as final layer 
%	 containing one output neuron per each word. 
%%	The multi-class classifier  is very common in deep neural networks. For example, in the natural language generation or graph embedding models, the final layers are  always implemented  as a softmax layer  with one output neuron per each word or node, which can be as large as the size of the whole dictionary or graph.  
%	It is prohibitively to calculate the intractable normalizing constant of softmax for each training data point. We analyze the well-known negative sampling and propose the amplified negative sampling algorithm, which gains higher performance with lower training cost. 
%	
	% explore the DNN frameworks to for internet embedding and  
	
	% However, in reality, we may find: the provided algorithm could overlook  or can hardly integrate some enlightening information within its framework; the training process can  be computationally intensive; and the expressing of complex logic computations could be complicated. 
%	In this thesis, we tackle those problems separately by novel information integration architectures, a sample-efficient training algorithm and a decalrative logical programming interface expressing complex logic.
	
	 
	% improve each component by demonstrating novel algorithms or expressive languages. 
	
	%Some of the bottlenecks for scalable analyzing involve the deficiency of proposed algorithm to integrate all the available data, the computationally intensive learning process of the algorithm, and the complicated development for   complex logic computations. In this thesis, we 
	
	%a well-designed architecture to integrate all the available features, an inexpensive  
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\makeintropages

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%In the first part, we aim to utilize the variants of recurrent neural network (RNN)  to incorporate some enlightening sequential information overlooked by the previous works in different domains and tasks. 
%We find, 
%	However, in reality, we may find the provided algorithm could overlook  or  hardly integrate some enlightening information within its framework. For e.g., 
%in Spoken Language Understanding, only the best of automatic speech recognition (ASR)  interpretations (hypotheses) for  an input audio signal is utilized to understand the intent, while the rest hypotheses containing fragmented important messages are ignored. We investigate a series of  methods to jointly utilize top $n$ interpretations by integrating the hypothesized text or hypothesis embedding vectors with BiLSTM and achieve significant accuracy improvements for intent or domain classification.   
%Similarly, while embedding  the Internet structure,  only the distances among IP address are utilized to build up the network coordinate system  but the information contained by the  IP address  is unnoticed. 
% Since the RNN and its variants have been proven to be effective to capture causal and/or contextural information from sequantial data, we extend them to incorporate the information from sequences of spoken words or IP bits. 
%We propose the DIP, a deep learning based framework for IP network representations, which normalizes each IP address to separate sequences by the volume of contained routable information within IP bits and exploits a variant of RNN for a low-dimensional representation.  

%In the second part, we investigate the training algorithm for multi-class classifiers with a large output-class size, which is common in deep neural networks and typicaly implemented as a softmax layer in the final layer 
%like natural language generation with  a softmax layer as final layer 
%containing one output neuron per each word. 
%	The multi-class classifier  is very common in deep neural networks. For example, in the natural language generation or graph embedding models, the final layers are  always implemented  as a softmax layer  with one output neuron per each word or node, which can be as large as the size of the whole dictionary or graph.  
%It is prohibitively to calculate the intractable normalizing constant of softmax for each training data point. We analyze the well-known negative sampling and propose the amplified negative sampling algorithm, which gains higher performance with lower training cost. 

% explore the DNN frameworks to for internet embedding and  

% However, in reality, we may find: the provided algorithm could overlook  or can hardly integrate some enlightening information within its framework; the training process can  be computationally intensive; and the expressing of complex logic computations could be complicated. 
%	In this thesis, we tackle those problems separately by novel information integration architectures, a sample-efficient training algorithm and a decalrative logical programming interface expressing complex logic.


% improve each component by demonstrating novel algorithms or expressive languages. 

%Some of the bottlenecks for scalable analyzing involve the deficiency of proposed algorithm to integrate all the available data, the computationally intensive learning process of the algorithm, and the complicated development for   complex logic computations. In this thesis, we 

%a well-designed architecture to integrate all the available features, an inexpensive  

\chapter{Introduction}


\section{Motivations}
In the big data era, we have witnessed the rising demand to efficiently and conveniently extracting insights  from large-scale data sets for decision making in different domains. The demand has driven researchers to propose various  neural network-based algorithms revolutionizing many fields, ranging from image processing \citep{he2016deep}, natural language processing \citep{devlin2018bert} to speech recognition \citep{amodei2016deep}, etc. In addition, the big data analysing and machine learning platforms  in open sorce and commercial markets  like PyTorch \citep{paszke2019pytorch}, Tensorflow~\citep{abadi2016tensorflow} and Apache Spark~\citep{zaharia2010spark} are continuously built up, to provide maximum flexibility and speed while implementing the analyzing pipeline with existing or user-defined algorithms. 
%For large data processing, some steps may 

Although the huge success has been achieved, for the advanced analytics on scalable data sets, there are still some challenges and ongoing efforts to tackle them, including:
\begin{itemize}
	
	\item \textbf{Integrating more information.} While great efforts have been dedicated to assimilate massive amounts of data for analysing algorithms, only a tip of the big data iceberg has been utilized during analysing. 
	To make full use of the assimilated data, researchers design novel architectures of algorithms like the bi-directional RNN \citep{graves2013speech} to exploit the backward information and involving more tasks for training like the masked language model and next sentence prediction mentioned in Bert \citep{devlin2018bert}.
	
	In two of the fastest growing areas in  computer science, the internet embedding and spoken language understanding, we find some enlightening information not considered in the existing analyzing pipeline, which makes  the performance of corresponding downstream tasks non-optimal. Currently, while embedding the internet structure,  only a single source of structural data among IP addresses like hop counts ~\citep{barford-sigcomm,barford-infocom} is utilized to build up the network coordinate system. We realize the IP address of the host in internet, a sequence of bits, could provide a coarse indication of the location of the host, but is overlooked by current internet embedding techniques. As for understanding the speech, only the recognition result (hypothesis) of an input speech with highest ASR confidence score \citep{tur2011spoken} or reranking score \citep{peng2013search, morbini2012reranking} is relied and transferred to the natural language understanding (NLU) module for domain or intent classification.  We find the first best hypothesis can be noisy, while the other hypotheses can be more similar to the  ground-truth transcription of the speech. Driven by the above findings, we would like to propose novel frameworks involving the unnoticed information for a better performance in downstream tasks. 
	 % but the information contained by the  IP address sequences  is unnoticed. 
	
	\item \textbf{High training cost for large-class classification.} Among all the analysing algorithms, the neural networks with numerous layers and parameters are recently widely utilized  in different domains due to their strength at learning features at different levels of abstraction for a better decision making. However, the training could be quite computationally expensive especially when the final layer is a softmax layer for large-class classification (e.g. word embedding, graph embedding), since the cost of standard training algorithm  is proportional to the output class size (e.g. size of words in a dictionary, nodes in a graph).  To tackle the issue, a number of techniques like, negative sampling \citep{mikolov2013efficient}, hierarchical  softmax \citep{morin2005hierarchical}, adaptive softmax \citep{bengio2008adaptive,rawat2019sampled} , are developed.
	
	Negative sampling is one of the most popular techniques utilized in practice due to its simplicity and efficiency. Experimental observations show that  larger negative sample size can achieve a better downstream task performance. However, the training cost is more expensive for a larger sample size. Could we get the best of the two worlds? This dream motivates us to analyze the technique and further amplify the negative sampling to get a higher performance with unchanged or even smaller sample size. 
	
	%We careful analyze the technique, and find the implication of negative sampling. Based on 
	

	
	\item \textbf{Complicated expression of recursion.} Recursions are ubiquitous in advanced data analytics, such as graph analytics,  
	data mining algorithms. An efficient development of a complicated recursive algorithm on the well-known data analytics platforms like Apache Spark requires deep understanding of the algorithm and platform's libraries. To simplify the development,  a renaissance of interest has been brought to Datalog, a declarative logic programming language, for its succinct expression of recursions. Numerous of Datalog systems including DeALS \citep{yang2015parallel}, BigDatalog \citep{shkapsky2016big}, RaSQL \citep{gu2019rasql} are built up but there is still space for improvement on usability and interoperability.
	%the rule-based logical programming is considered. 
	%A series of declarative programming languages like 
	
	Most of the conventional Datalog systems adopt the Datalog syntax, which requires a deep understanding of logical programming. Recently, this is realized by RaSQL and the RaSQL propose a simple extension to  SQL syntax to improve the usability.  However, for a wider audience from data science community, should it be better to  provide a  data scientists' familiar DataFrame-based high-level API encapsulating a wide range of Datalog algorithms like Spark MLlib and support  the API in different languages like PySpark? For a flexible developing, is it possible to support the logical operations within a DataFrame-like  data structure? These two questions stimulate us for a better Datalog-based data analytics tool design.

\end{itemize}

\section{Contributions}
All the contributions of this thesis center around  the aforementioned motivations and summarized as follows:
\begin{itemize}
	\item We realize and experimentally show  the structural information contained in IP addresses for internet embedding and propose a deep learning based framework, DIP, to utilize the information. To the best of our knowledge, DIP is the first framework to predict distance (or hop count) to arbitrary IPs (even unknown hosts i.e., not contained in training data) based only on the value of their IP address and routable prefix without any other domain knowledge.
	\item We further explore the impact of deep learning in the network  security area by using network embeddings to learn IP maps for spoofing detection. We  combine the DIP with the hop count filtering, a well-known map-based spoofing detection mechanism, which maps each IP to a hop count value for one target IP address and is restricted to the detection on specific targets. The new framework can help any Internet server detect packets spoofed with any IP address without any additional measurements to that IP.
	\item We pioneer the spoken language understanding research on jointly utilizing all hypotheses from ASR module. We investigate the exact matching between hypotheses and the ground-truth transcription of the input speech, which reveals the value contained in the 2$^{nd}-n^{th}$ best hypotheses. To involve more than one hypothesis during NLU, we introduces a series of simple yet efficient models and significantly improve the SLU system robustness to the noises from  ASR module. 
	\item We take the effort to efficiently train the high computational cost multi-class classifiers. We propose a new sample-efficient training algorithm, amplified negative sampling (ANS). We theoretically and experimentally demonstrate the ANS leads to the higher-accuracy model of a larger sample size without paying its high computational cost.
	\item Finally, we design a cross-language (Python, Scala, or Java) Datalog programming interface with two important components, LLib and LFrame. LLib is the high-level logical library, providing the encapsulation of a wide range of Datalog algorihtms.  LFrame is an extension to DataFrame with the functionality of basic logical operations defined by us like  definitions of recursive rules. With running examples, we show the simplified development of recursive applications   and flexible collaborations between LLib or LFrame and exisiting Spark libraries with the help of expressive LLib and LFrame.
\end{itemize}



\section{Thesis Outline}

In this dissertation, we mainly address the three of the most important elements of the efficient scalable data analyzing. We firstly explore how to make full use of all information while extracting semantics and insights for decision making. More specifically, in  Chapter \ref{ch:dip}: (a) We propose the DIP, a variant of RNN, for establishing the network coordinate system via embedding the IP address sequence. Compared to the coordinate system built on single source of structural data like latency or hop count, we demonstrate the superiority of DIP for unknown IPs; (b) We show the spoofing detection, one of a wide range of problems the DIP framework can be applicable to, and the benefits of combination with DIP on   significantly reduced cost of achieving complete IP maps. In Chapter \ref{ch:nbest}, we explore the value of unused ASR interpretations and numerous ways to integrate them in the  spoken language understanding system.   

Secondly, we explore how a 
  large-class classifier can be  efficiently trained through a sample-efficient training algorithm in Chapter \ref{ch:nec}. In the chapter, the intuition, theoretical and experimental analysis of the  developed algorithm, amplified negative sampling, are  discussed.  Then, we explore how to design a succinct interface for advanced data analytics with superiority on recursion expressions in Chapter \ref{ch:lib}. We develop a high-level Datalog library, LLib,  for simplified end-to-end recursive application development with existing Datalog algorithms and a data structure, LFrame, for flexibly defining logic of a new recursive application. We finally conclude the dissertation and talk about the future work in Chapter \ref{ch:con}.
  
  

%The remainder of the dissertation is organized as follows. In Chapter \ref{ch:dip}, 



\chapter{Extracing Latent  Information from IP Network}
\label{ch:dip}

\input{dip}

\chapter{Extracing Latent Information from Abandoned Speech Interpretations}
\label{ch:nbest}

\input{speech}

\chapter{Amplified Negative Sampling: A Sample-Efficient Training Algorithm }
\label{ch:nec}
\input{negativeSampling}

\chapter{Expressive Library for Recursive Queries: LLib and LFrame}
\label{ch:lib}

\input{llib}


\chapter{Conclusion}
\label{ch:con}

In this dissertation, we have presented models to extract information with involvement of unused data for the optimal downstream task performance. We also discussed the approaches to improve the efficiency of  developing analytics algorithms with complex recursions and training large-class classifier for the big data. In this section, we will wrap up the dissertation and suggest some avenues for future research.
%training deep neural networks efficiently for large-class classifier 

In Chapter \ref{ch:dip}, we discussed the way to include the IP address sequence in the internet embedding by deep learning, which we found could tackle the issue of missing information for locating unknown IP addresses and provide sufficient coverage of  protected networks in denial-of-service attacks. We discovered the hidden structural information encoded in a node's IPv4 address. To extract the encoded information, we designed a deep learning based framework, DIP, which is a ten-layer neural network and a variant of RNN. During the training of DIP, we jointly utilized the IP address, hop count and routing information and used the distance estimation as the objective. With experiments on  test data, we found the learned embedding vectors  could preserve the real-world clustering of the associated nodes and predict distance between them accurately. Moreover, for unknown hosts, DIP could accurately impute hop count distance to them merely by their IP addresses and routable prefixes. These findings inspired us to apply DIP on hop count filtering based spoofing detection, a classical framework in network security. We reviewed the previous design of spoofing detection with the explicitly computed IP maps, the collection of  immutable structural network properties  among IPs like hop counts.  Since DIP could learn the embedding of internet and predict the structural properties of arbitrary IPs, the DIP-based detection mechanism could save the time of measuring the structural properties to build up  IP maps and increase the coverage of protected hosts. 

In Chapter \ref{ch:nbest}, we discussed the way to include the 2$^{nd}$-$n^{th}$ best hypotheses generated by ASR module in SLU pipeline, which we found  could improve the SLU system robustness to ASR errors. In a conventional SLU system, the ASR module transcribes the input speech to sentences (hypotheses)  and the hypothesis with highest confidence score will be transferred to natural language understanding module. We argued that solely relying on the best hypothesis could be erroneous and provided the spoken recognition quality distribution. We reviewed the existing approaches to utilize $n$ hypotheses, among which  the reranking model is the most popular one. With motivating examples, we demonstrated the condition that reranking model cannot figure out. To tackle the issue, we tried integrating the hypothesized texts and embedding vectors in numerous models. Among the developed models, the PoolingAvg, which concatenates the embedding vectors and uses a average pooling layer to generate a unified vector, outperformed all the others.  The PoolingAvg achieved significant classification accuracy improvement for downstream tasks including domain classification and intent classification. We also observed that with more hypotheses combined, the performance could be further improved. 

In Chapter \ref{ch:nec}, we addressed the expensive computational cost to train a large-class classifier for a big data set. When a classifier is modeled as a neural network, it is always be represented by a softmax layer. The softmax layer training is the main reason for the high cost due to its intractable normalization constant. We gave a thorough overview of  the  negative sampling, hierarchical softmax, adaptive softmax to approximate the softmax.  We proposed the amplified negative sampling by introducing the amplifing factor to the known negative sampling. With experiments on real-world datasets and tasks, we showed the efficiency of the amplified negative sampling on both the sampling cost savings and performance  boosting. 

In Chapter \ref{ch:lib}, we demonstrated an expressive interface for succinct development of complex analytics with numerous recursions.  We introduced the superiority of Datalog systems on expressing recursive queries and overviewed the latest Datalog systems, including BigDatalog, RaSQL. Although much effort has been taken to optimize the scalable logical operations  within distributed Datalog environment and  design a better interface to extend the expressive power, there is still space to improve the usability considering  "normal" data analysts. We provide the cross-language libraries, i.e. LLib and LFrame. The LLib, similar to Spark MLlib,  encapsulates common Datalog applications for an end-to-end development. The LFrame, an extension to Spark DataFrame, supports both relational and logical operations. With running examples, we showed the interface helped data scientists  efficiently develop succinct recursive analyzing algorithms with a familiar environment.


All together, we are really excited about all the progress made in the big data analytics systems and algorithms  and glad to be able to contribute to this. We do think there is still a long way to go and would like to point out the avenues for future research in our mind.

\textbf{More available but unused information.}  In the dissertation, we introduce the unrecognized information in conventional algorithms of two domains, however we believe this is common in other domains. Here, we would like to keep sharing our other observations of  the two domains  and hope these could encourage more upcoming research works in those areas or more. 
 
As for the SLU, the first type of information can be helpful is the acoustic-model information like confidence score, which is ignored in our current hypotheses integration models but shown to be informative in other speech applications \citep{kumar2014normalization, fiscus1997post}. The recognized hypotheses from ASR module are associated with confidence scores, which tell the quality of each hypothesis. The confidence scores exist in different  layers, for example, the  confidence score of $i^{th}$ best hypothesis or the  confidence score of the $j^{th}$ word in the $i^{th}$ best hypothesis. The PoolingAvg approach treats each hypothesis equally although the quality of the hypotheses actually varies. We thus need to consider a new design to hierarchically involve the multi-layer acoustic-model information for a more efficient integration. The second direction is to use deep learning framework on word lattice \citep{liu2014efficient} or confusion network \citep{hakkani2006beyond, tur2002improving}. The hypotheses are derived from the word lattice or confusion network, so they may contain more information like times.  

As for the internet embedding, since we have proposed the DIP and the neural network can be easily extended for other data sources, we could use the other latency measurements \citep{vivaldi} more than hop counts by simply changing the cost function of the DIP.  In addition, the AS membership information could provide a coarse indication of locality of IPs \citep{barford-infocom}. We could adapt the AS membership as another estimating error within the internet embedding algorithm.

\textbf{Multi-task learning.}  Besides involving the extra information, there is also opportunity to consider more relevant tasks by multi-task learning (MTL),  which generalizes the learned model and broadcasts the knowledge among multiple fields. MTL \citep{zhang2017survey,liu2019multi,caruana1997multitask} is a widely used machine learning paradigm for training multiple related tasks in the same time. The superiority of MTL is to avoid  overfitting and transfer knowledge. This could inspire us to add more tasks for the model training. For example, in  SLU,  we can consider a new task to reconstruct the ground-truth transcription using the hypotheses.  The common tasks for hypothesis embedding model are natural language understanding tasks (domain or intent classification or slot filling), while the transcription reconstruction can help recover the error contained in hypotheses for a better understanding.

\textbf{Multi-platform data analytics.}  Our LLib and LFrame could support data scientists' familiar development with multiple programming languages, while another need to support multi-platform programs keeps increasing. For example, while detecting abnormal exchanges of stock market with real-time platforms such as Spark Streaming \citep{zaharia2013discretized}, users may be interested in retrieving historical stock data stored in a NoSQL databases like MongoDB \citep{chodorow2013mongodb}. To support the cross-storage-system queries, the ploystore \citep{duggan2015bigdawg} architecture was proposed. Following this trend, for more succinct recursive query analysis, we could try to build a Datalog-based polystore system, which should be extensible to various platforms like Spark, MongoDB, etc.

I hope this dissertation could inspire the research in deep learning and scalable  data analytics. Furthermore, I hope the embedding approaches, training algorithms and libraries constructed could contribute to both academic and industrial applications. 




\phantomsection
\addcontentsline{toc}{chapter}{Bibliograpy}

\bibliographystyle{apa}
\bibliography{thesis}    % bibliography references

\end{document}

